{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-31T17:21:58.137276Z","iopub.execute_input":"2022-03-31T17:21:58.137601Z","iopub.status.idle":"2022-03-31T17:21:59.093559Z","shell.execute_reply.started":"2022-03-31T17:21:58.137516Z","shell.execute_reply":"2022-03-31T17:21:59.092868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindata = '/kaggle/input/limit-orderbook-data/Train_Dst_NoAuction_DecPre_CF_7.txt'\ncvdata = '/kaggle/input/limit-orderbook-data/Test_Dst_NoAuction_DecPre_CF_9.txt'","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:21:59.095114Z","iopub.execute_input":"2022-03-31T17:21:59.095368Z","iopub.status.idle":"2022-03-31T17:21:59.100568Z","shell.execute_reply.started":"2022-03-31T17:21:59.095335Z","shell.execute_reply":"2022-03-31T17:21:59.099804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nBATCH = 128\n\n\"\"\"\n    DataLoader file fot the LOB dataset gives out the lob matrix of size (batch_size, 1, Seq_len, 4*levels)\n    Data can is taken from FI-2010 Dataset with default horizon prediction level = 50 (147, columns)\n    Horizon = 10 (Level = 144) \n    For custom Lobster Data set we we change the horizon according to the dataset that we have defined  \n\"\"\"\n\n\nclass GenMatData(Dataset):\n    def __init__(self, path, level=10, horizon=147, sequencelen=100):\n        metadata = np.loadtxt(path).T\n        self.levels = level\n        self.sequence_len = sequencelen\n        self.train_data = metadata[:, :4*level]\n        self.predictions = metadata[:, horizon]\n\n    def __len__(self):\n        return self.train_data.shape[0] - self.sequence_len + 1\n\n    def __getitem__(self, item):\n        lob_instance = torch.tensor(self.train_data[item:item+self.sequence_len, :]).float()\n        out = torch.tensor(self.predictions[item+self.sequence_len-1])\n        return lob_instance.unsqueeze(0), out-1\n\n\ndef LobDataLoader(batch, lobpath, shuf, level=10, seq_len=100, Horizon=147):\n    dataset = GenMatData(path=lobpath, level=level, horizon=Horizon, sequencelen=seq_len)\n    in_loader = DataLoader(dataset, shuffle=shuf, batch_size=batch, pin_memory=True)\n    return in_loader\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:21:59.102191Z","iopub.execute_input":"2022-03-31T17:21:59.102557Z","iopub.status.idle":"2022-03-31T17:22:00.375205Z","shell.execute_reply.started":"2022-03-31T17:21:59.102521Z","shell.execute_reply":"2022-03-31T17:22:00.374474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_loader = LobDataLoader(batch=BATCH, lobpath=traindata, shuf=True)\n# cv_loader = LobDataLoader(batch=BATCH, lobpath=cvdata, shuf=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:22:00.37764Z","iopub.execute_input":"2022-03-31T17:22:00.37814Z","iopub.status.idle":"2022-03-31T17:22:00.382115Z","shell.execute_reply.started":"2022-03-31T17:22:00.378102Z","shell.execute_reply":"2022-03-31T17:22:00.38129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass BaseCNN(nn.Module):\n    def __init__(self, in_chanel, out_chanel, kernel, strides):\n        super(BaseCNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_chanel, out_chanel, kernel, bias=False, stride=strides)\n        self.norm1 = nn.BatchNorm2d(out_chanel)\n        self.norm2 = nn.BatchNorm2d(out_chanel)\n        self.norm3 = nn.BatchNorm2d(out_chanel)\n        self.conv2 = nn.Conv2d(out_chanel, out_chanel, kernel_size=(3, 1), padding='same')\n        self.conv3 = nn.Conv2d(out_chanel, out_chanel, kernel_size=(3, 1), padding='same')\n        self.l_relu = nn.LeakyReLU(negative_slope=3e-2, inplace=True)\n\n    def forward(self, mat):\n        out = self.l_relu(self.norm1(self.conv1(mat)))\n        out = self.l_relu(self.norm2(self.conv2(out)))\n        out = self.l_relu(self.norm3(self.conv3(out)))\n        return out\n\n\nclass DynamicConv(nn.Module):\n    def __init__(self, in_chanel, out_chanel, dims):\n        super(DynamicConv, self).__init__()\n        \"\"\"\n                We can initialize the linear layer of the model in one other way by using \n                nn.adaptiveavgpool2d from nn and thus compute the attention in that manner \n                reducing the model weight. \n                in_chanel : input chanel in dynamic conv layer \n                out_chanel : number of output chanel \n                dim : dimension (L*W) of the input matrix \n        \"\"\"\n        self.dim = dims\n        self.dconv1 = nn.Conv2d(in_chanel, out_chanel, kernel_size=(1, self.dim[1]), padding='same', bias=False)\n        self.dconv2 = nn.Conv2d(in_chanel, out_chanel, kernel_size=(5, self.dim[1]), padding='same', bias=False)\n        self.dconv3 = nn.Conv2d(in_chanel, out_chanel, kernel_size=(3, self.dim[1]), padding='same', bias=False)\n        self.linear = nn.Linear(self.dim[0]*self.dim[1]*in_chanel, 3)\n        self.leaky = nn.LeakyReLU(negative_slope=1e-2, inplace=True)\n        self.bnorm1 = nn.BatchNorm2d(out_chanel)\n        self.bnorm2 = nn.BatchNorm2d(out_chanel)\n        self.bnorm3 = nn.BatchNorm2d(out_chanel)\n        self.soft = nn.Softmax(dim=1)\n\n    def forward(self, mat):\n        weight = self.soft(self.linear(mat.view((mat.size(0), -1))))\n        out1 = self.leaky(self.bnorm1(self.dconv1(mat))).unsqueeze(1)\n        out2 = self.leaky(self.bnorm2(self.dconv2(mat))).unsqueeze(1)\n        out3 = self.leaky(self.bnorm3(self.dconv3(mat))).unsqueeze(1)\n        out = torch.cat((out1, out2, out3), dim=1)\n        out = torch.sum(torch.mul(out, weight.reshape(weight.size(0), weight.size(1), 1, 1, 1)), dim=1)\n\n        return out\n\n\nclass DeepLob(nn.Module):\n    def __init__(self, in_chanels, hidden_size, num_layers, seq_length):\n        super(DeepLob, self).__init__()\n        self.one = BaseCNN(in_chanel=in_chanels, out_chanel=32, kernel=(1, 2), strides=(1, 2))\n        self.two = BaseCNN(in_chanel=32, out_chanel=64, kernel=(1, 2), strides=(1, 2))\n        self.three = BaseCNN(in_chanel=64, out_chanel=128, kernel=(1, 10), strides=(1, 10))\n        self.dyconv = DynamicConv(in_chanel=128, out_chanel=128, dims=(100, 1))\n        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n        self.liner = nn.Sequential(nn.Linear(seq_length*hidden_size, hidden_size), nn.ReLU(), # Just giving last LSTM node to Dense UNIT\n                                   nn.Linear(hidden_size, 3))\n\n    def forward(self, mat):\n        out_lob = self.one(mat)\n        out_lob = self.two(out_lob)\n        out_lob = self.three(out_lob)\n        out_lob = self.dyconv(out_lob)\n        out_lob = out_lob.squeeze(-1)\n        out_lob = torch.transpose(out_lob, 1, 2)\n        out_lob, (ho, co) = self.lstm(out_lob)\n        out_lob = out_lob.reshape(out_lob.size(0), -1)\n        return self.liner(out_lob)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:22:00.383563Z","iopub.execute_input":"2022-03-31T17:22:00.384054Z","iopub.status.idle":"2022-03-31T17:22:00.409199Z","shell.execute_reply.started":"2022-03-31T17:22:00.384018Z","shell.execute_reply":"2022-03-31T17:22:00.408519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ndef accu(model, loader):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    accuracy = []\n    model.eval()\n    with torch.no_grad():\n        for mat, label in loader:\n            mat = mat.to(device)\n            label = label.to(device)\n            pre = model(mat)\n            _, pre = torch.max(pre, dim=1)\n            accuracy.append((pre==label).sum()/label.size(0))\n        accuracy = torch.tensor(accuracy).mean()\n        return accuracy.item()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:22:00.412654Z","iopub.execute_input":"2022-03-31T17:22:00.412988Z","iopub.status.idle":"2022-03-31T17:22:00.448247Z","shell.execute_reply.started":"2022-03-31T17:22:00.41296Z","shell.execute_reply":"2022-03-31T17:22:00.447574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nfrom tqdm import trange, tqdm\ndef train():\n    EPOCHS = 20\n    traindata = '/kaggle/input/limit-orderbook-data/Train_Dst_NoAuction_DecPre_CF_7.txt'\n    cvdata = '/kaggle/input/limit-orderbook-data/Test_Dst_NoAuction_DecPre_CF_9.txt'\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    loader = LobDataLoader(batch=256, lobpath=traindata, shuf=True)\n    cvloader = LobDataLoader(batch=256, lobpath=cvdata, shuf=True)\n    model = DeepLob(in_chanels=1, hidden_size=128, num_layers=2, seq_length=100)\n    model = model.to(device)\n    opt = optim.Adam(params=model.parameters(), lr=2e-3)\n    crit = nn.CrossEntropyLoss()\n    for epoch in range(EPOCHS):\n        print('EPOCH {}'.format(epoch))\n        for data, labels in tqdm(loader):\n            data = data.to(device)\n            labels = labels.to(device)\n            prediction = model(data)\n            loss = crit(prediction, labels.long())\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n        print('Epoch {} training Accuracy {} CV accuracy {}'.format(epoch, accu(model, loader), accu(model, cvloader)))\n        model.train()            \n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:22:00.449709Z","iopub.execute_input":"2022-03-31T17:22:00.450082Z","iopub.status.idle":"2022-03-31T17:22:00.459871Z","shell.execute_reply.started":"2022-03-31T17:22:00.450047Z","shell.execute_reply":"2022-03-31T17:22:00.459085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T17:22:00.462317Z","iopub.execute_input":"2022-03-31T17:22:00.462553Z","iopub.status.idle":"2022-03-31T19:57:29.736913Z","shell.execute_reply.started":"2022-03-31T17:22:00.462529Z","shell.execute_reply":"2022-03-31T19:57:29.735457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"76 accuracy Model : \nwith Dyconv 2 = kernel = 5 \n\n\n77.8 accu Model only taking last output from LSTM layer ","metadata":{}},{"cell_type":"code","source":"# lloader = LobDataLoader(batch=32, lobpath=traindata, shuf=True)\n# for data, lablel in lloader:\n#     print(data.shape)\n#     print(label.shape)\n#     break","metadata":{"execution":{"iopub.status.busy":"2022-03-31T19:57:29.738323Z","iopub.execute_input":"2022-03-31T19:57:29.738592Z","iopub.status.idle":"2022-03-31T19:57:29.742151Z","shell.execute_reply.started":"2022-03-31T19:57:29.738556Z","shell.execute_reply":"2022-03-31T19:57:29.741213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}